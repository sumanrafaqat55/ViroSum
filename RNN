RNN
Step 1
!pip install biopython



Step 2
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from Bio import SeqIO


Step 3
# Function to encode sequences into one-hot encoding
def one_hot_encode(sequence, max_length):
    encoding = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'G': [0, 0, 0, 1]}
    encoded_seq = [encoding.get(base, [0, 0, 0, 0]) for base in sequence]
    # Padding sequences to make them the same length
    return np.array(encoded_seq[:max_length] + [[0, 0, 0, 0]] * (max_length - len(sequence)))

# Load sequences from the FASTA file
def load_sequences(fasta_file):
    sequences = [str(seq_record.seq) for seq_record in SeqIO.parse(fasta_file, "fasta")]
    return sequences

fasta_file = "/content/1000 sequence of NS4A HCV text file.txt"  # Replace with your actual FASTA file path
sequences = load_sequences(fasta_file)
max_length = max(len(seq) for seq in sequences)

# One-hot encode all sequences
encoded_sequences = np.array([one_hot_encode(seq, max_length) for seq in sequences])


Step 4
# Prepare sequences for training
X = encoded_sequences[:-1]  # All sequences except the last one
y = encoded_sequences[1:]   # All sequences except the first one (target sequence is the next one)

# Reshape data for RNN input
X = np.array(X)
y = np.array(y)

# Ensure data shapes are correct
print(X.shape, y.shape)


Step 5
# Build the RNN model
def build_rnn_model(input_shape):
    model = Sequential()
    model.add(SimpleRNN(128, input_shape=input_shape, return_sequences=True))  # RNN layer with return_sequences=True
    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting
    model.add(SimpleRNN(128, return_sequences=True))  # Another RNN layer with return_sequences=True
    model.add(Dropout(0.5))  # Dropout layer
    model.add(Dense(4, activation='softmax'))  # Output layer with 4 possible outputs (A, T, C, G)
    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Define input shape
input_shape = (X.shape[1], 4)  # Sequence length and number of features (A, T, C, G)
model = build_rnn_model(input_shape)

model.summary()


Step 6
# Build the RNN model (keeping the same architecture)
def build_rnn_model(input_shape):
    model = Sequential()
    model.add(SimpleRNN(128, input_shape=input_shape, return_sequences=True))  # RNN layer with return_sequences=True
    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting
    model.add(SimpleRNN(128, return_sequences=True))  # Another RNN layer with return_sequences=True
    model.add(Dropout(0.5))  # Dropout layer
    model.add(Dense(4, activation='softmax'))  # Output layer with 4 possible outputs (A, T, C, G)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])  # Using categorical_crossentropy
    return model

# Define input shape
input_shape = (X.shape[1], 4)  # Sequence length and number of features (A, T, C, G)
model = build_rnn_model(input_shape)

model.summary()

# Train the model with categorical_crossentropy loss
model.fit(X, y, epochs=40, batch_size=32, validation_split=0.2)


Step 7
def generate_sequence(model, seed_sequence, length, max_length):
    generated_sequence = seed_sequence  # Initialize with the seed sequence
    for _ in range(length):
        # Ensure we are reshaping generated_sequence to the correct shape (1, max_length, 4)
        input_sequence = generated_sequence[-max_length:].reshape(1, max_length, 4)  # Use the last `max_length` sequence

        # Predict the next nucleotide (amino acid) as a class index (0, 1, 2, or 3)
        pred = model.predict(input_sequence)  # Make prediction using the model
        next_base = np.argmax(pred, axis=-1)[0, -1]  # Get the predicted class for the last base

        # Convert the predicted class into a one-hot encoded vector
        next_base_one_hot = np.eye(4)[next_base]  # Convert integer to one-hot encoding

        # Reshape next_base_one_hot to ensure it's a 3D array of shape (1, 1, 4) for concatenation
        next_base_one_hot = next_base_one_hot.reshape(1, 1, 4)

        # Append the predicted base to the sequence
        generated_sequence = np.concatenate((generated_sequence, next_base_one_hot), axis=1)  # Concatenate along the second axis (sequence length)

    return generated_sequence


Step 8
# Generate 20 sequences
generated_sequences = []
for _ in range(20):
    seed_sequence = random.choice(sequences)  # Choose a random starting sequence
    encoded_seed = one_hot_encode(seed_sequence, max_length).reshape(1, max_length, 4)  # One-hot encode the seed
    generated_seq = generate_sequence(model, encoded_seed, length=100, max_length=max_length)  # Generate a sequence of length 100
    generated_sequences.append(generated_seq)

# Print the generated sequences
for i, seq in enumerate(generated_sequences):
    # Convert the generated one-hot encoded sequence back to the original base sequence
    seq_str = ''.join(['ATCG'[np.argmax(base)] for base in seq[0]])  # `seq[0]` to access the sequence in the batch
    print(f"Generated Sequence {i + 1}: {seq_str}")

